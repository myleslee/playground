12 月 1 日傍晚 6 时左右有用户反馈云引擎服务异常，部分请求出现 502 应答，我们工程师立刻上线排查，最终确认是平台的问题，并及时进行了修复，使服务在 6 点 43 分恢复正常。这里将这起事故的细节给大家汇报一下，由此给大家业务带来的影响，我们深表歉意。

故障时间：12 月 1 日 18 点 03 分 ～ 18 点 43 分

故障影响：国内节点的所有云引擎应用出现间歇性 502 响应，没有使用云引擎的应用不受影响。

故障原因：
先简单介绍一下云引擎的前端架构。我们使用容器技术，来实现云引擎的弹性扩展，从客户端发到云引擎的所有请求都是先落到负载均衡服务器，然后途径路由服务器（负责将请求动态匹配到正确的 docker 节点），最后才到各个应用节点来实际处理。
现在有应用在使用云引擎＋ webSocket 的组合方式实现自己的业务逻辑，这次故障发生的时候，该应用的 webSocket 出现了大量的连接异常。我们的路由服务器对 webSocket 的异常处理有缺陷，导致服务进程异常退出。路由服务随即被监控程序启动，然后再次接到 webSocket 异常请求而退出，如此震荡。在此过程中其他应用的请求会因为路由服务的重启而出现间歇性 502 响应。

解决办法：
1，增强路由服务的鲁棒性，完善对 webSocket 的异常处理（已完成）；
2，加强路由服务进程的主动监控，力争第一时间发现问题（待做）；
